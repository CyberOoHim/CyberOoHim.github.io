<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Simplified Taigi POJ/Hanji/English Tokenizer</title>
<style>
    body {
        font-family: sans-serif;
        font-size: 1.2em;
        line-height: 1.6;
        margin: 20px;
    }
    h1 {
        font-size: 2.5em;
        margin-bottom: 3px;
    }
    .help-upload-wrapper {
        margin-bottom: 0px;
        display:inline-block;
    }
     .help-upload-wrapper button {
         float:left;
      }
    .help-upload-wrapper #uploadDict {
        float:left;
       margin-left: 10px;
    }

    #uploadDict{
     font-size: 1.4em;
     display: inline-block;
    }
    h2, h3 {
        font-size: 1.8em;
        margin-bottom: 3px;
        margin-top: 3px;
    }
    textarea {
        font-size: 1.2em;
        padding: 10px;
        border: 1px solid #ccc;
        margin-bottom: 8px;
    }
    button {
        font-size: 1.2em;
        margin-right: 10px;
        padding: 10px 15px;
        border: 1px solid #ddd;
        background-color: #eee;
    }

    .token {
        font-size: 1.2em;
        padding: 2px;
        margin: 2px;
    }
    .hanji { background-color: #e6f3ff; }
    .poj { background-color: #fff0e6; }
    .english { background-color: #e6ffe6; }
    .unknown { background-color: #f0f0f0; }

    .hanji-result {
        font-size: 1.2em;
        line-height: 1.5;
        margin-bottom: 8px;
    }
    .output-section {
        margin-top: 20px;
    }

    .output-separator{
        margin: 10px 0;
        border-bottom: 1px solid #ccc;
    }

    p{
        font-size: 1.2em;
    }
     .pos-hidden {
        display: none;
    }
  .help-message {
        clear:both;
        border: 1px solid #ddd;
        padding: 10px;
        margin-top: 0px;
        background-color: #f9f9f9;
        display: none;
    }
     .help-message p {
        font-size: 1.2em;
        line-height: 1.4;
    }

</style>
</head>
<body>
<h1>Simplified Taigi Han-lô/English Tokenizer</h1>
      <div class="help-upload-wrapper">
            <button id="helpButton">Help</button>
          <input type="file" id="uploadDict" accept=".csv" />
      </div>
       <div id="helpMessage" class="help-message">
            <p>This tokenizer tool can process text containing a mix of Taigi POJ, Hanji, and English. It uses a built-in dictionary, and you can upload an additional dictionary for custom terms. The "Tokenize" button will split your text into tokens, and the dictionary lookup below will display matches and POS/Definitions. "Copy" will copy the processed result, and you can use "Hide 詞性/解說" to hide or show the POS/definitions. </p>
        </div>
    <div class="input-section">
        <h3>Enter Taigi/English Text</h3>
        <textarea id="pojInput" rows="4" style="width: 100%" placeholder="POJ, Hanji, English, or a mix..."></textarea>
        <div class="button-container">
            <button onclick="processText()">Tokenize</button>
            <button id="togglePosButton">Hide 詞性/解說</button>
             <button id="copyButton">Copy</button>

    <div class="output-section">
        <div id="combinedOutput"></div>
    </div>
  
<script>
    // Configuration: Set dictionary path, GitHub pages or local folder
    const dictionaryPath = "kautian_詞目+義項_20250116_POJ_trimmed.csv";
    const folderPath = "/assets/csv";
    const github_pages = false;
    const dict_to_fetch = github_pages ? folderPath + "/" + dictionaryPath : dictionaryPath;

    let showPos = true; // Initial state for showing POS/Definitions

    class MixedTokenizer {
        constructor() {
            this.dictionary = {};
             this.englishIndex = {}; // New index for English terms
            this.pojChars = ['Á', 'À', 'Â', 'Ā', 'A̍', 'É', 'È', 'Ê', 'Ē', 'E̍', 'Í', 'Ì', 'Î', 'Ī', 'I̍', 'Ḿ', 'M̀', 'M̂', 'M̄', 'M̍', 'Ń', 'Ǹ', 'N̂', 'N̄', 'N̍', 'Ó', 'Ò', 'Ô', 'Ō', 'O̍', 'O͘', 'Ó͘', 'Ò͘', 'Ô͘', 'Ō͘', 'O̍͘', 'Ú', 'Ù', 'Û', 'Ū', 'U̍', 'á', 'à', 'â', 'ā', 'a̍', 'é', 'è', 'ê', 'ē', 'e̍', 'í', 'ì', 'î', 'ī', 'i̍', 'ḿ', 'm̀', 'm̂', 'm̄', 'm̍', 'ń', 'ǹ', 'n̂', 'n̄', 'n̍', 'ⁿ', 'ó', 'ò', 'ô', 'ō', 'o̍', 'o͘', 'ó͘', 'ò͘', 'ô͘', 'ō͘', 'o̍͘', 'ú', 'ù', 'û', 'ū', 'u̍'];
            this.pojCharRegex = this._createPojCharRegex();
            this.ignoredCharsRegex = /[\d!"#$%&'()*+,./:;<=>?@[\\\]^_`{|}~\u3000-\u303F\uFF00-\uFFEF&&[^\u002D\u2010]]/;
            // UPDATED Hanji Regex (including extension blocks)
            this.hanjiRegex = /[\u4e00-\u9fff\u3400-\u4dbf\u{20000}-\u{2A6DF}\u{2A700}-\u{2B73F}\u{2B740}-\u{2B81F}\u{2B820}-\u{2CEAF}\u{2CEB0}-\u{2EBEF}\u{30000}-\u{3134F}]/u;
            this.loadBuiltInDictionary();
        }
          _createPojCharRegex() {
            const pojCharPattern =
                "[AEIOUaeiou][\\u030d\\u0300\\u0301\\u0302\\u0304\\u0358\\u207F]*|" +
                "[\\u004F\\u006F][\\u030d\\u0358]*|" +
                "[\\u00D3\\u00F3][\\u0358]*|" +
                "[\\u00D2\\u00F2][\\u0358]*|" +
                "[\\u00D4\\u00F4][\\u0358]*|" +
                "[\\u014C\\u014D][\\u0358]*|" +
                "[\\u004E\\u006E][\\u0302\\u0304]*|" +
                "[\\u004D\\u006D][\\u0300\\u0302\\u0304]*|" +
                "[\\u006E][\\u207F]*|" +
                "[\\u00C1\\u00C0\\u00C2\\u0100\\u00C9\\u00C8\\u00CA\\u0112\\u00CD\\u00CC\\u00CE\\u012A\\u1E3E\\u0143\\u01F8\\u00D3\\u00D2\\u00D4\\u014C\\u00DA\\u00D9\\u00DB\\u016A\\u00E1\\u00E0\\u00E2\\u0101\\u00E9\\u00E8\\u00EA\\u0113\\u00ED\\u00EC\\u00EE\\u012B\\u1E3F\\u0144\\u01F9\\u00F3\\u00F2\\u00F4\\u014D\\u00FA\\u00F9\\u00FB\\u016B]|" +
                "[ -͘]|" +
                "[a-zA-Z]";
            return new RegExp(pojCharPattern);
        }

         async loadBuiltInDictionary() {
            try {
                const response = await fetch(dict_to_fetch);
                if (!response.ok) {
                    throw new Error(`Failed to load dictionary: ${response.status} ${response.statusText}`);
                }
                const text = await response.text();
                this.parseDictionaryContent(text);
                console.log("Dictionary loaded successfully!");
            } catch (error) {
                console.error("Error loading built-in dictionary:", error);
            }
        }
      parseDictionaryContent(text) {
            const rows = text.split('\n').map(row => row.split(',').map(cell => cell.trim()));

            for (const row of rows) {
                if (row.length < 2) continue;

                const [first, second, pos = ''] = row;
                if (!first || !second) continue;

                // Handle Taigi-English format
                if (this._isTaigiEnglish(row)) {
                    const taigi = first;
                    const english = second;

                    // Add Taigi to English mapping (with pos)
                    if (!this.dictionary[taigi]) {
                        this.dictionary[taigi] = [[], []];
                    }
                    if (!this.dictionary[taigi][0].includes(english)) {
                        this.dictionary[taigi][0].push(english);
                        this.dictionary[taigi][1].push({ english: english, pos: pos });
                    }

                    // Add English to Taigi mapping (NOW WITH pos)
                    if (!this.dictionary[english]) {
                        this.dictionary[english] = [[], []];
                    }
                    if (!this.dictionary[english][0].includes(taigi)) {
                        this.dictionary[english][0].push(taigi);
                        // Store pos along with the Taigi word
                        this.dictionary[english][1].push({ taigi: taigi, pos: pos });
                    }


                    // Add to English index (case-insensitive)
                    const lowerEnglish = english.toLowerCase();
                    if (!this.englishIndex[lowerEnglish]) {
                        this.englishIndex[lowerEnglish] = new Set();
                    }
                    this.englishIndex[lowerEnglish].add(english);
                }
                // Handle traditional POJ-Hanji format
                else {
                    const hanji = this._isHanjiChar(first.charAt(0)) ? first : second;
                    const poj = this._isHanjiChar(first.charAt(0)) ? second : first;

                    // Add Hanji to POJ mappings with POS/Definition
                    if (!this.dictionary[hanji]) {
                        this.dictionary[hanji] = [[], []];
                    }
                    const pojVariants = poj.split('/');
                    pojVariants.forEach(variant => {
                        variant = variant.trim();
                        if (!this.dictionary[hanji][0].includes(variant)) {
                            this.dictionary[hanji][0].push(variant);
                        }
                        
                        if (!this.dictionary[hanji][1].some(entry => entry.poj === variant)) {
                            this.dictionary[hanji][1].push({ poj: variant, pos: pos });
                        }
                    });

                    // Add POJ to Hanji mappings with POS/Definition
                    pojVariants.forEach(p => {
                        const key = p.trim();
                        if (!this.dictionary[key]) {
                           this.dictionary[key] = [[], []];
                        }
                        if (!this.dictionary[key][0].includes(hanji)) {
                            this.dictionary[key][0].push(hanji);
                        }
                        if (!this.dictionary[key][1].some(entry => entry.hanji === hanji)) {
                            this.dictionary[key][1].push({ hanji: hanji, pos: pos });
                        }
                     });
                }
            }
        }

          _isTaigiEnglish(row) {
              const [first, second] = row;
            // Check if it's English (allowing spaces and hyphens)
            const isEnglish = /^[a-zA-Z\s-]+$/.test(second);
            // Check if it's Taigi (contains POJ characters or follows POJ patterns)
            const isTaigi = this.pojChars.some(char => first.includes(char)) || 
                           /^[a-zA-Z\s\-]+$/.test(first);
            return isTaigi && isEnglish;
        }


         tokenizePoj(text) {
            const tokens = [];
            let i = 0;
            const textLength = text.length;

            while (i < textLength) {
                let matched = false;

                // Skip Hanji characters
                if (this.hanjiRegex.test(text[i])) {
                    i++;
                    continue;
                }

                // Skip ignored characters
                if (this.ignoredCharsRegex.test(text[i])) {
                    i++;
                    continue;
                }

                // Try dictionary matches first (longest match)
                for (let length = Math.min(textLength - i, 20); length > 0; length--) {
                    const candidate = text.slice(i, i + length);

                    // Skip if candidate contains Hanji or ignored characters
                    if (this.hanjiRegex.test(candidate) || this.ignoredCharsRegex.test(candidate)) continue;

                    // Try both original case and swapped case for the first character
                    const firstChar = candidate.charAt(0);
                    const swappedFirstChar = this._swapCase(firstChar);
                    const swappedCaseCandidate = swappedFirstChar + candidate.slice(1);

                    // Check both cases in the dictionary
                     if (this.dictionary[candidate] || this.dictionary[candidate.toLowerCase()] || this.dictionary[swappedCaseCandidate]) {
                       const dictKey = this.dictionary[candidate] ? candidate :
                        this.dictionary[candidate.toLowerCase()] ? candidate.toLowerCase() : swappedCaseCandidate;
                        tokens.push({
                            text: candidate,
                            dictionary: this.dictionary[dictKey],
                            type: 'poj'
                        });
                        i += length;
                        matched = true;
                        break;
                    }
                }

                if (!matched) {
                    // Handle individual characters if no dictionary match found
                    const char = text[i];

                    if (!this.ignoredCharsRegex.test(char) && !this.hanjiRegex.test(char)) {
                        const isPOJ = this.pojCharRegex.test(char);
                        tokens.push({
                            text: char,
                             dictionary: null,
                            type: isPOJ ? 'poj' : 'non-poj'
                        });
                    }
                    i++;
                }
            }

            return this.pojFilterTokens(tokens);
        }
            // Helper function to swap the case of a character
        _swapCase(char) {
            if (char >= 'A' && char <= 'Z') {
                return char.toLowerCase();
            } else if (char >= 'a' && char <= 'z') {
                return char.toUpperCase();
            } else {
                return char;
            }
        }

        pojFilterTokens(tokens) {
            const filtered = [];
            let i = 0;

            while (i < tokens.length) {
                const token = tokens[i];

                // Handle special diacritical marks
                if (['͘', '̍', '̍͘', 'ⁿ'].includes(token.text)) {
                    if (filtered.length > 0) {
                        // Combine with previous token
                        const prevToken = filtered[filtered.length - 1];
                        prevToken.text += token.text;

                        // Update dictionary matches for combined token
                        if (this.dictionary[prevToken.text]) {
                            prevToken.dictionary = this.dictionary[prevToken.text];
                        } else if (this.dictionary[prevToken.text.toLowerCase()]) {
                             prevToken.dictionary = this.dictionary[prevToken.text.toLowerCase()];
                        }
                    } else {
                        // If no previous token, add as is
                        filtered.push(token);
                    }
                } else {
                    filtered.push(token);
                }
                i++;
            }
             return filtered;
        }

        tokenizeHanji(text) {
          const tokens = [];
            let i = 0;
            while (i < text.length) {
                let matched = false;
                // Start from the longest possible match and go down to single characters
                for (let length = Math.min(text.length - i, 10); length >= 1; length--) {
                    let candidate = '';
                    let current = i;
                    let addedChars = 0;

                    // Build the candidate string, handling surrogate pairs
                    for (let j = 0; j < length; j++) {
                        if (current >= text.length) break; // Prevent index out of bounds

                        const codePoint = text.codePointAt(current);
                        const char = String.fromCodePoint(codePoint);
                        candidate += char;

                        current += codePoint > 0xFFFF ? 2 : 1;
                        addedChars += codePoint > 0xFFFF ? 2 : 1;
                    }

                   if (this.dictionary[candidate]) {
                        tokens.push({
                            'text': candidate,
                            'dictionary': this.dictionary[candidate],
                            'type': 'hanji'
                        });
                        i += addedChars; // Increment by the actual number of characters/surrogate pairs
                        matched = true;
                        break; // Exit the inner loop once a match is found
                    }
                }
              if (!matched) {
                    const codePoint = text.codePointAt(i);
                    const char = String.fromCodePoint(codePoint);

                    if (this._isHanjiChar(char)) {
                        tokens.push({
                            'text': char,
                            'dictionary': null,
                            'type': 'hanji'
                        });
                    }
                    i += codePoint > 0xFFFF ? 2 : 1; // Increment by 2 for surrogate pairs
                }
            }
             return tokens;
        }

        tokenize(text) {
            const tokens = [];
            let i = 0;
            while (i < text.length) {
                let matched = false;
                
                // Try to match the longest possible token
                for (let len = Math.min(30, text.length - i); len > 0; len--) {
                    const candidate = text.slice(i, i + len);
                    
                    // Skip if contains ignored characters
                    if (this.ignoredCharsRegex.test(candidate)) continue;

                    // Check if it's an English word
                    if (/^[a-zA-Z\s-]+$/.test(candidate)) {
                        const lowerCandidate = candidate.toLowerCase();
                        if (this.englishIndex[lowerCandidate]) {
                            // Use the original case if it exists in dictionary
                            const originalCase = Array.from(this.englishIndex[lowerCandidate])[0];
                            tokens.push({
                                text: originalCase,
                                type: 'english',
                                dictionary: this.dictionary[originalCase]
                            });
                            i += len;
                            matched = true;
                            break;
                        }
                    }
                    
                    // Check if it's in dictionary
                   if (this.dictionary[candidate]) {
                        const type = this._isHanjiChar(candidate[0]) ? 'hanji' : 'poj';
                        tokens.push({
                            text: candidate,
                            type: type,
                            dictionary: this.dictionary[candidate]
                        });
                        i += len;
                        matched = true;
                        break;
                    }
                }

                if (!matched) {
                    // Handle single character
                    const char = text[i];
                    let type = 'unknown';
                    if (this._isHanjiChar(char)) type = 'hanji';
                    else if (this._isPojChar(char)) type = 'poj';
                    else if (/^[a-zA-Z]$/.test(char)) type = 'english';

                    tokens.push({
                        text: char,
                        type: type,
                        dictionary: this.dictionary[char]
                    });
                    i++;
                }
            }
            return this.filterTokens(tokens);
        }
     // Modified filterTokens to handle edge cases better
    filterTokens(tokens) {
        const filteredTokens = [];
        let i = 0;

        while (i < tokens.length) {
            const token = tokens[i];

            // Skip empty tokens
            if (!token || token.text.trim() === '') {
                i++;
                continue;
            }

            // Handle double hyphen cases
            if (i + 2 < tokens.length &&
                tokens[i].text === '-' &&
                tokens[i + 1].text === '-') {

                const nextToken = tokens[i + 2];
                if (nextToken && (nextToken.type === 'poj' || nextToken.type === 'hanji')) {
                    const combinedToken = {
                        text: '--' + nextToken.text,
                        type: nextToken.type,
                         dictionary: nextToken.dictionary
                    };
                    filteredTokens.push(combinedToken);
                    i += 3;
                    continue;
                }
            }

            // Don't add duplicate tokens in sequence
            const prevToken = filteredTokens[filteredTokens.length - 1];
            if (!prevToken || prevToken.text !== token.text) {
                filteredTokens.push(token);
            }
            i++;
        }

        return filteredTokens;
    }
        _isHanjiChar(char) {
            return this.hanjiRegex.test(char);
        }

        _isPojChar(char) {
            return this.pojCharRegex.test(char);
        }
    }

    const mixedTokenizer = new MixedTokenizer();

function processText() {
    try {
        const input = document.getElementById('pojInput').value;
        if (!input.trim()) {
            document.getElementById('combinedOutput').innerHTML = '<p>Please enter some text to process</p>';
            return;
        }

        const tokens = mixedTokenizer.tokenize(input);
        // Add spaces between tokens in output
        let tokenOutput = '';
       tokens.forEach((token, index) => {
            let displayText = token.text;
            // Replace sequences of hyphens longer than 2 with '--' for display
            if (displayText.startsWith('--') && displayText.length > 2) {
                let counter = 0;
                while (displayText.charAt(counter) === '-') {
                    counter++;
                }
                if (counter > 2) {
                    displayText = '--' + displayText.substring(counter);
                }
            }
            // Add space after each token except the last one
             tokenOutput += `<span class="token ${token.type}">${displayText}</span>${index < tokens.length - 1 ? ' ' : ''}`;
        });

        // Track processed tokens to avoid duplicates in dictionary lookup
        const processedTokens = new Set();
        let dictOutput = '';

        tokens.forEach(token => {
            const tokenKey = `${token.type}:${token.text}`;
            if (processedTokens.has(tokenKey)) return;

            const dictionary = token.dictionary;
            if (dictionary && dictionary[1].length > 0) {
                const entries = dictionary[1];
                const matchString = entries.map((entry, i) => {
                    const posText = entry.pos ? ` <span class="pos-info" ${showPos ? '' : 'style="display:none;"'}>(${entry.pos.replace(/"/g, '')})</span>` : '';
                    if (entry.english) {
                        return `${i + 1}. ${entry.english} ${posText}`;
                    } else if (entry.hanji) {
                        return `${i + 1}. ${entry.hanji} ${posText}`;
                    } else if (entry.poj) {
                        return `${i + 1}. ${entry.poj} ${posText}`;
                     } else if (entry.taigi) {
                        return `${i + 1}. ${entry.taigi} ${posText}`;
                    }
                }).filter(s => s).join('; ');

                if (matchString) {
                    dictOutput += `
                    <div class="hanji-result">
                        <strong>${token.text}</strong> → ${matchString}
                    </div>`;
                }
            }
            processedTokens.add(tokenKey);
        });

        document.getElementById('combinedOutput').innerHTML = `
            <h3>Tokenized Text:</h3>
            ${tokenOutput}
            <div class='output-separator'></div>
            <h3>Dictionary Lookup</h3>
            ${dictOutput || '<p>No matches found</p>'}
        `;

    } catch (error) {
        console.error('Error processing text:', error);
        document.getElementById('combinedOutput').innerHTML =
            `<p>Error processing text: ${error.message}</p>`;
    }
}
       document.getElementById('copyButton').addEventListener('click', function() {
    const input = document.getElementById('pojInput').value;
    const tokenElements = document.querySelectorAll('#combinedOutput .token');

    // Filter out unrecognized characters (like emojis)
    let tokenText = Array.from(tokenElements).map(el => {
        if (el.classList.contains('unknown')) {
            return ''; // Return empty string for unknown tokens
        } else {
            return el.textContent; // Use textContent for other tokens
        }
    }).join(' ').replace(/\s+/g, ' ').trim(); // Replace multiple spaces with a single space

    const dictElements = document.querySelectorAll('#combinedOutput .hanji-result');
    let dictText = Array.from(dictElements).map(el => {
        const token = el.querySelector('strong').textContent;
        let resultText = "";

        Array.from(el.childNodes).forEach(node => {
            if (node.nodeType === Node.TEXT_NODE) {
                resultText += node.textContent;
            } else if (node.tagName === 'STRONG') {
                return; // Skip the <strong> node itself
            } else if (node.classList && !node.classList.contains('pos-info')) {
                resultText += node.textContent;
            } else if (showPos && node.classList && node.classList.contains('pos-info')) {
                resultText += node.textContent;
            }
        });

        resultText = resultText.replace(/^ → /, '').trim();

        return `${token} ${resultText}`;
    }).join('\n');

    // If dictText is empty, provide a "No matches found" message
    if (!dictText) {
        dictText = "No matches found.";
    }

    const copyText = `<Taigi Text>\n\n${input}\n---\n<Tokenized Text>\n\n${tokenText}\n---\n<Dictionary Lookup>\n\n${dictText}\n---`;

    navigator.clipboard.writeText(copyText).then(() => {
        this.textContent = 'Copied!';
        setTimeout(() => {
            this.textContent = 'Copy';
        }, 1500);
    }).catch(err => {
        console.error('Failed to copy: ', err);
        this.textContent = 'Copy failed';
        setTimeout(() => {
            this.textContent = 'Copy';
        }, 1500);
    });
});
      document.getElementById('togglePosButton').addEventListener('click', function() {
        showPos = !showPos; // Toggle state
          const posInfoElements = document.querySelectorAll('.pos-info');
        posInfoElements.forEach(element => {
            element.style.display = showPos ? 'inline' : 'none';
        });
          this.textContent = showPos ? 'Hide 詞性/解說' : 'Show 詞性/解說';

      });

    document.getElementById('uploadDict').addEventListener('change', function(event) {
        const file = event.target.files[0];
        if (!file) {
            alert('No file selected.');
            return;
        }

        const reader = new FileReader();
        reader.onload = function(e) {
            try {
                mixedTokenizer.parseDictionaryContent(e.target.result);
                alert('Additional dictionary loaded successfully!');
            } catch (error) {
                console.error('Error parsing additional dictionary:', error);
                alert('Error parsing dictionary. Check the format.');
            }
        };
        reader.onerror = function(e) {
            console.error('Error reading file:', e);
            alert('Error reading file.');
        };
        reader.readAsText(file);
    });
    
       document.getElementById('helpButton').addEventListener('click', function() {
        const helpMessage = document.getElementById('helpMessage');
        const computedStyle = window.getComputedStyle(helpMessage);
        helpMessage.style.display = computedStyle.display === 'none' ? 'block' : 'none';
    });
    
</script>
</body>
</html>

